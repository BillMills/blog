---
layout: post
title: "Next Steps"
subtitle: ""
date: 2015-08-17
permalink: next-steps
---

Today was my last day at the Mozilla Science Lab, after a year as their first Community Manager. It was a privilege to be part of Kaitlin Thaney's team during this time; the Science Lab started a long list of global-scale conversations in open science that are organizing thought and catalyzing action among a panoply of scientific stakeholders. These conversations, and the things I learned from Mozilla and the Science Lab team, are of huge value to the future of science, and I encourage you to find a way to learn from them, too. I can articulate now what I could only sense before, and that clarity is so liberating.

After much reflection, I decided to take that clarity and return to active research in particle and nuclear physics. Folding these ideas back into the day-to-day practice of *doing science* requires involvement, access and presence to dig into the details and follow the threads of implementation that differ from field to field and lab to lab. Following these threads to their esoteric extremes is going to be necessary to get open science over the finish line. But it's not the Science Lab's role to dive down every rabbit hole; that task is left to us.

## Open Nuclear Physics

In the short term, I'm excited to head back to TRIUMF as a research software developer, to look into extending some work I did there as a postdoc. I have a lot more to say about the emerging field of research software development, but I'll save that for its own post later this week. Right now, I want to dig into what a first pass at open nuclear physics can look like, at TRIUMF and more broadly.

There are a [ton][thread] of skills and ideas that surround open science, but I'd like to start by focusing on a few cardinal features: open access, open source, and open data.

#### Open Access

My [favorite particle physics paper][arxiv] ever wasn't about the Higgs, or even dark matter - it's about the tremendous citation advantages that high energy physics, nuclear's next door neighbor, has enjoyed thanks to publishing on the arxiv for the last twenty years. Combine this with the fact that there are [thousands][prc] of *Physical Review C* papers with preprints on arxiv, setting a pretty clear precedent, and that *Journal of Physics G* [actively encourages][jopg] the upload of reviewed nuclear physics manuscripts to the arxiv, and the practical barriers to open access publishing in nuclear physics are pretty much zero. Furthermore, essentially all experimental facilities in nuclear physics are run by governing bodies that can impose values and criteria on the experiments proposed to them. As a colloquial practice, we're most of the way there already; I'd like to see nuclear labs go the last mile, and **mandate preprint submission to the arxiv** for all papers written on experiments conducted using their hardware, as part of the experimental proposal and review process.

#### Open Source

I have too many things to say about scientific open source to cram them all into this blog post - I'll be discussing some more in depth strategies later this week. Something to address early as code takes a leading role in the doing of science, is **software citation**. In order to start creating a knowable landscape of what's out there in terms of open source code, we need to start reporting that code *in context* with the science it is supporting; that way, we can begin to defeat the software discoverability problem by staying abreast of the scientific literature, and thereby take better advantage of the improvements to productivity and code quality that open source collaboration brings. Furthermore, a standard of software citation is necessary for the growing field of scientific software development to define and integrate itself within the existing academic and scientific establishment.

A [lively working group][swcite] on software citation is amassing observations and thoughts on the practice over at FORCE11; our colleagues in astronomy are [actively investigating software citation standards][astro] in a conversation that could serve as a useful precedent in nuclear. Furthermore, the last time I was at TRIUMF, I introduced the GRIFFIN Collaboration to open source software development on GitHub, a practice which they have retained to this day, and which paves the way for stamping our software with DOIs, thanks to the wonderful [service][zenodo] provided by Zenodo and GitHub. Therefore, I would like to see **DOI minting and citation adopted as a standard practice** on GRIFFIN, following an investigation of a sensible format. All the pieces are on the table, and demand for this recognition is mounting; all that's needed is a largely cost-free willingness to participate.

#### Open Data

Open data presents perhaps the most challenges for researchers: fearing the scoop, struggling with privacy concerns, making data understandable to third parties. And yet, like open access, there is a [well documented citation advantage][data] to publishing data. GRIFFIN is in a good position to address all of the concerns around data raised above:

 - GRIFFIN data is, as a matter of policy, deleted after 5 years - it's just too big to keep around. If you're deleting your data, you have certainly forgone all possibility of publishing anything further on it.
 - Like all data in nuclear and particle physics, GRIFFIN data obeys a strict, machine generated standardized format. If this data is released under an open license, I will personally write and distribute an appropriate parser to make sure the data is easily readable by third parties.
 - Metadata needed to make sense of GRIFFIN data, particularly post-sorting, is relatively simple to communicate; beam and target composition, detector geometry and efficiency and trigger state and performance should be enough to understand the sorted data.

What's more, our colleagues over at the LHC have recently adopted an [open data program][lhc], releasing data after several years, accompanied by enough software infrastructure and context to make sense of the data - this program can serve as a precedent and a model for GRIFFIN. The only hurdle left then, is the scale of GRIFFIN data, which can be produced at up to 200 TB a *week*. While it's not immediately clear how to distribute data at this scale without an unrealistic burden on the lab, one thing we can do today is **assign an open license to GRIFFIN data after five years**. As it stands, this policy is completely academic; as noted above, the other thing that happens after five years, is GRIFFIN data is deleted to make space on our servers. Nevertheless, baking in a CC-0 (or otherwise) license at this point opens the door to distribution of data by third parties or means not currently available to us, without costing the lab anything in terms of resources or opportunity, and helps position TRIUMF as a leader in open data policy.

#### Sounds easy, right?

Wrong. I'm sure there will be nuances and speedbumps, and digging into those details is exactly what I was talking about in the introduction to this post; please, let me know your criticisms and ideas on how we can meet the challenges I laid out above in the [comments][issue]. I'm looking forward to seeing what progress we can make on these fronts at TRIUMF, and to digging deeper into the role of scientific software development - more on that quest later this week, and these initiatives as they evolve.

[thread][https://github.com/mozillascience/studyGroupLessons/issues/7]
[arxiv][http://arxiv.org/abs/0906.5418]
[prc][http://arxiv.org/find/grp_physics/1/jr:+AND+C+AND+Phys+Rev/0/1/0/all/0/1]
[jopg][http://iopscience.iop.org/0954-3899/page/Why%20publish%20with%20JPhysG]
[data][https://peerj.com/articles/175/]
[issue][]
[swcite][https://www.force11.org/group/software-citation-working-group]
[astro][http://astronomy-software-index.github.io/2015-workshop/]
[zenodo][https://guides.github.com/activities/citable-code/]
[lhc][http://opendata.cern.ch/research]









## Scientific Software Developers

Programming continues to move towards [center stage][ssi] in all fields of research, and this growing primacy is creating a generation of grad students and post docs like myself: classically trained scientists who find themselves thrust into roles that are mostly or entirely coding. My colleagues and I are and will continue to demand recognition for our contributions, and by virtue of experience and necessity, we are creating a new role in the lab: that of the Scientific Software Developer. This role will come into focus over the next few years, which means that now is the time to think about who and what we want these new scientists to be.

This is a crucial conversation for the open science movement. Without mindfulness, scientific software developers may narrow their focus to their most obvious responsibility: writing quality scientific software. But the lesson we need to learn from Mozilla's Working Open Guide, and from collaborations like NCEAS' Codefest, CERN's Webfest, and the Global Sprint is that the *ways and contexts* in which we write that software will dramatically impact how open that code is, how sharable and collaborative it is, and by extension how much impact this new subfield has. Scientific software developers have the opportunity to break their fields of research out of their silos - but only if we are willing to embody a number of meta-skills exceeding code alone.

#### Code Curation

One of the key meta-responsibilities of the scientific software developer is to provide their lab with an awareness of what code is already out there for their use - in other words, to defeat the discoverability problem scientific software currently suffers from, by being able to provide our colleagues with informed recommendations on how to take advantage of the open source landscape. Without this awareness, we will continue to work in isolation, reinventing wheels in our separate labs and failing to leverage the power of the open source movement to accelerate and improve our coding. We can do this by creating a knowable landscape of relevant software within a field, and curating that knowledge within our institutions.

A 'knowable landscape', in this case, means a multi-front conversation about what we're all hacking on at the moment. It consists of a few parts:

 - **Software Citation.** In order to break our code out of institutional silos, we need a way to discover it *in context* with the research it supports. That way, by staying abreast of the current literature in our fields (as one does in any case), we can develop a sense of the open source marketplace as it is affecting current research. 
 - **Conference talks & tracks.** In addition to citing our software after the fact, we need to start discussing it as it is developed, so we can help each other adapt early. Traditional software developers have a rich ecology of software conferences to discover what's new; scientific software developers need the same.

#### End-To-End Openness

Something Abby Cabunoc Mayes does an excellent job of championing over at Mozilla is the concept of 'working open', which she describes in detail in the similarly-named Guide: it won't be enough for scientific software developers to take their code and slap it up on GitHub after the fact - we need to include our colleagues in the open source development process from the very beginning, and keep them on board from start to finish.

By keeping our colleagues involved in an open conversation around a development process that incorporates their input, we will create opportunities to fold in their ideas for how to make our projects as useful as possible, which multiplies the impact those software projects can have. But what's more, including people in a well-managed project helps them gain a sense of *ownership* of the code, and communicating throughout the process helps *win hearts and minds* as people watch, become familiar with, and participate in these projects. In contrast, parachuting a completed project onto people who have never heard of it before is a tough sell; relevance and usage aren't clear, and the up-front costs of integrating a new piece of software are too daunting without cultivated enthusiasm to carry people over the hump. The Working Open Guide treats these ideas in depth, but here's a few things to start:

 - **Onramps & Roadmaps.** By providing a clear roadmap of a project (starting perhpas with the 10 minute plans I wrote about a few months ago), we give our colleagues the context and clarity to offer feedback early; by making clear contributing guidelines, communicating future goals and curating simple issues for new contributors to tackle, we pave the way for people to get involved in our development projects - and it's those people who will champion the project in future in their home institutions, extending the impact and value of our work.
 - **Consistent Communication.** By leveraging things like twitter and a professional blog, as well as encouraging conversation in our issue trackers, scientific software developers can ensure that a conversation about their work is both visible and accessible throughout the development process, feeding all of the goals so far discussed: these communication channels both support the discoverability of our work, and help include as wide an audience as possible in its development.

#### Capacity Building

I love node.js. I could pretend to make a bunch of technical or strategic arguments for its use, but honestly: I just think its fun, and left to my own devices, I'll use it for a lot of my server-side needs, which is exactly what I did during my last post doc. Trouble was, I was the only JavaScript developer in that lab, and when I left, all that work might as well have been advanced technology from a lost and inscrutable alien empire - interesting, possibly powerful, but unusable and unmaintainable in the hands it fell into. In order for our work as scientific software developers to have the maximum use, we need to help support a program of capacity building in our labs and institutions. That way, our colleagues will be better able to use, adapt and maintain our code without our direct involvement - and so we can focus on building code *with* them, rather than building code *for* them. 

In order to mitigate the around-the-block help request lineups many of us entertain, there are a few programs scientific software developers should involve themselves in:

 - **Software Carpentry** needs little introduction at this point - every cohort at every lab and institution should have this workshop made available to them as they begin their terms. Arranging a workshop is not especially difficult; a scientific software developer should shoot to throw one at least once a year.
 - **Study Groups** are the program I founded at Mozilla this year: regular, casual get-togethers for researchers to share skills and work together on problems and challenges. By helping organize an ongoing conversation and skill-building space like a Study Group, we create opportunities for our colleagues to discover and participate in what we're working on, and build the skills necessary to participate effectively.

 Code curation, procedural openness, and capacity building are things I intend to hammer on in my career as a scientific software developer; I believe that these meta-skills are what will allow us to create a rennaisance of scientific programming, rather than simply shoulder the overflow programming labor from our colleagues.


[ssi][http://www.software.ac.uk/blog/2014-12-04-its-impossible-conduct-research-without-software-say-7-out-10-uk-researchers]